{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter notebook for Optimizing an Azure ML Pipeline \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Hyperparameter Tuning for a chosen model [Logistic Regression]\n",
    "### Initiate a _workspace_ and start an _experiment_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "gather": {
     "logged": 1713875269107
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing interactive authentication. Please follow the instructions on the terminal.\n",
      "Interactive authentication successfully completed.\n",
      "Workspace name: quick-starts-ws-258212\n",
      "Azure region: westeurope\n",
      "Subscription id: 976ee174-3882-4721-b90a-b5fef6b72f24\n",
      "Resource group: aml-quickstarts-258212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code L7J5Y82RR to authenticate.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace, Experiment\n",
    "\n",
    "# Get the Workspace and Experiment objects running \n",
    "ws = Workspace.from_config()\n",
    "\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')\n",
    "\n",
    "exp_name = \"UCnavAMLproject01\"\n",
    "exp = Experiment(workspace=ws, name=exp_name)\n",
    "\n",
    "run = exp.start_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and configure a compute cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "gather": {
     "logged": 1713875985217
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing cluster found! \n",
      "\n",
      "\n",
      "Running\n",
      "Compute Cluster details: \n",
      " {'errors': [], 'creationTime': '2024-04-23T12:17:20.868209+00:00', 'createdBy': {'userObjectId': '7ed85bb2-82b4-4698-b957-ed49ef858ced', 'userTenantId': '660b3398-b80e-49d2-bc5b-ac1dc93b5254', 'userName': 'ODL_User 258212'}, 'modifiedTime': '2024-04-23T12:18:52.122137+00:00', 'state': 'Running', 'vmSize': 'Standard_DS3_v2'}\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "\n",
    "cluster_name = \"UCnavProject01\"\n",
    "\n",
    "# Create a compute cluster\n",
    "# Use vm_size = \"Standard_DS3_v2\" in provisioning_configuration.\n",
    "# max_nodes should be no greater than 4.\n",
    "\n",
    "try: \n",
    "    compute_cluster = ComputeTarget(workspace = ws, name = cluster_name)  # checking for an existing compute cluster\n",
    "    print('Existing compute cluster found! \\n')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute cluster... \\n')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='Standard_DS3_v2', min_nodes=0, max_nodes=4)\n",
    "    compute_cluster = ComputeTarget.create(workspace = ws, name = cluster_name, provisioning_configuration=compute_config)  \n",
    "\n",
    "# Display the status and details of the Compute resources \n",
    "compute_cluster.wait_for_completion(show_output = True)\n",
    "print('\\n Compute Cluster details: \\n', compute_cluster.get_status().serialize()) \n",
    "\n",
    "compute_resources = ws.compute_targets\n",
    "for resource_name, resource_type in compute_resources.items():\n",
    "    print('\\n', resource_name, resource_type.type, resource_type.provisioning_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598275789986
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "from azureml.train.sklearn import SKLearn\n",
    "from azureml.train.hyperdrive.run import PrimaryMetricGoal\n",
    "from azureml.train.hyperdrive.policy import BanditPolicy\n",
    "from azureml.train.hyperdrive.sampling import RandomParameterSampling\n",
    "from azureml.train.hyperdrive.sampling import BayesianParameterSampling\n",
    "from azureml.train.hyperdrive.runconfig import HyperDriveConfig\n",
    "from azureml.train.hyperdrive.parameter_expressions import choice, uniform\n",
    "from azureml.core import Environment, ScriptRunConfig\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Sampler specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive.parameter_expressions import uniform, loguniform, choice, quniform\n",
    "\n",
    "# Specify parameter sampler\n",
    "\n",
    "# RandomParameterSampling\n",
    "ps = RandomParameterSampling(\n",
    "    {\n",
    "        \"learning_rate\": uniform(0.01, 1),\n",
    "        \"C\": loguniform(0.01, 10),\n",
    "        \"batch_size\": choice(16, 32, 64, 128, 256),\n",
    "        \"hidden_size\": choice(80, 120, 160, 240, 480), \n",
    "        \"max_iter\": quniform(100, 1000, 100),\n",
    "        \"solver\": choice(\"lbfgs\", \"newton-cg\", \"sag\", \"saga\")        \n",
    "    }\n",
    ")\n",
    "\n",
    "# BayesianParameterSampling\n",
    "ps_B = BayesianParameterSampling(\n",
    "    {'--learning-rate': uniform(0.01,1), \n",
    "     '--batch_size': choice(16, 32, 64, 128), \n",
    "     '--max_iter': choice(100, 150, 200, 250, 300) \n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy for early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify a Policy\n",
    "\n",
    "# policy for RandomParameterSampling \n",
    "policy_R = BanditPolicy(slack_factor=0.1, evaluation_interval=1, delay_evaluation=5)\n",
    "\n",
    "# policy for BayesianParameterSampling      \n",
    "## Bayesian sampling does not support early termination policies, so it's better to set the policy to \"None\"\n",
    "policy_B = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the environment for the training run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if \"training\" not in os.listdir():\n",
    "#   os.mkdir(\"./training\")\n",
    "    \n",
    "# Setup environment for the training run\n",
    "sklearn_env = Environment.from_conda_specification(name='sklearn-env', file_path='conda_dependencies.yml')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and register the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.data.dataset_factory import TabularDatasetFactory\n",
    "from azureml.core.dataset import Dataset\n",
    "from azureml.exceptions import UserErrorException\n",
    "\n",
    "# Create TabularDataset using TabularDatasetFactory\n",
    "## Data is available at: \n",
    "## \"https://automlsamplenotebookdata.blob.core.windows.net/automl-sample-notebook-data/bankmarketing_train.csv\"\n",
    "\n",
    "datapath_url = 'https://automlsamplenotebookdata.blob.core.windows.net/automl-sample-notebook-data/bankmarketing_train.csv'\n",
    "dataset = TabularDatasetFactory.from_delimited_files(path = datapath_url, random_state = 42)\n",
    "ds_name = 'UCbankmarketing'\n",
    "\n",
    "dataset_registered = False\n",
    "\n",
    "try:\n",
    "    temp = Dataset.get_by_name(workspace = ws, name = ds_name)\n",
    "    dataset_registered = True\n",
    "    \n",
    "except UserErrorException:\n",
    "    print(\"The Bank marketing dataset is not registered in workspace yet.\")\n",
    "\n",
    "if not dataset_registered:\n",
    "    print('Registering the dataset')\n",
    "    dataset = dataset.register(workspace = ws,\n",
    "                               name = ds_name,\n",
    "                               description = 'Udacity Bank marketing dataset',\n",
    "                               create_new_version = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an `estimator` for the _training_ script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a Logistic Regression estimator\n",
    "\n",
    "# from azureml.train.sklearn import LogisticRegression\n",
    "\n",
    "# estimator_LR = LogisticRegression(\n",
    "#     source_directory = \".\",              # Specify the training script directory\n",
    "#     compute_target = compute_cluster,    # Specify the compute target\n",
    "#     entry_script = \"train.py\",           # Specify the training script\n",
    "#     max_iter = 1000,                     # Set a large value for max iterations\n",
    "#     vm_size = \"Standard_DS3_v2\", \n",
    "#     vm_priority = \"dedicated\"\n",
    "# )\n",
    "\n",
    "# NOTE: \n",
    "## WARNING:azureml.train.sklearn:'SKLearn','LogisticRegression' estimators are deprecated. \n",
    "## Use 'ScriptRunConfig' from 'azureml.core.script_run_config' with own defined environment \n",
    "## or the AzureML-Tutorial curated environment.\n",
    "### script_run_config.script_run_config.target = cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig\n",
    "\n",
    "# Create a ScriptRunConfig Object to specify the configuration details of the training job\n",
    "\n",
    "args = ['--dataset', dataset.as_mount()]  # for mounting the dataset\n",
    "\n",
    "estimator_src = ScriptRunConfig(\n",
    "    source_directory = '.',               # Specify the training script directory\n",
    "    compute_target = compute_cluster,     # Specify the compute target\n",
    "    entry_script = \"train.py\",            # Specify the training script\n",
    "    environment = sklearn_env,            # Use the defined environment\n",
    "    arguments = args,                     # Additional/Optional command line arguments\n",
    "    max_iter = 1000,                      # Set a large value for max iterations\n",
    "    vm_size = \"Standard_DS3_v2\", \n",
    "    vm_priority = \"dedicated\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a `HyperDriveConfig`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive.runconfig import HyperDriveConfig\n",
    "\n",
    "# Create a HyperDriveConfig using the src object, hyperparameter sampler, and policy.\n",
    "hyperdrive_config = HyperDriveConfig(\n",
    "    estimator = estimator_src,\n",
    "    hyperparameter_sampling = ps,\n",
    "    policy = policy_R,                                # early_stopping_policy,\n",
    "    primary_metric_name = \"accuracy\",                 # Specify your primary metric\n",
    "    primary_metric_goal = PrimaryMetricGoal.MAXIMIZE,\n",
    "    max_total_runs = 20,\n",
    "    max_concurrent_runs = 4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the `HyperDriveConfig` to run the _experiment_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit the hyperdrive run to the experiment and show run details with the widget.\n",
    "hyperdrive_run = exp.submit(config = hyperdrive_config, show_output = True)\n",
    "\n",
    "RunDetails(hyperdrive_run).show()\n",
    "\n",
    "#hyperdrive_run.get_status()\n",
    "hyperdrive_run.wait_for_completion(show_output = True)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the best hyper-parameters for the model and save the best model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598276310862
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Get the best run and save the model from that run.\n",
    "\n",
    "best_run = hyperdrive_run.get_best_run_by_primary_metric()\n",
    "print('Best Run', best_run)\n",
    "print(\"Details :\", hyperdrive_best_run.get_details())\n",
    "print('\\n')\n",
    "best_run_metrics = best_run.get_metrics()\n",
    "parameter_values = best_run.get_details()['runDefinition']['arguments']\n",
    "print(\"Best Run file names: \", best_run.get_file_names())\n",
    "print('\\n')\n",
    "print(\"Best Run metrics: \", best_run_metrics)\n",
    "print('\\n')\n",
    "print('Best Run Id: ', best_run.id)\n",
    "print('\\n Accuracy: ', best_run_metrics['Accuracy'])\n",
    "print('\\n Learning Rate: ', parameter_values)\n",
    "\n",
    "#Save the best model.\n",
    "from azureml.core.model import Model\n",
    "\n",
    "best_model = best_run.register_model( model_name = 'hyperdrive_best_model', \n",
    "                                      model_path = './outputs/model.pkl',      # model_path='outputs/model.joblib'\n",
    "                                      model_framework = Model.Framework.SCIKITLEARN, \n",
    "                                      model_framework_version = sklearn.__version__,\n",
    "                                      tags = {\"Method\" : \"HyperDrive\"},\n",
    "                                      properties = {\"Accuracy\" : best_run_metrics[\"Accuracy\"]}\n",
    "                                    )\n",
    "\n",
    "joblib.dump(parameter_values, filename = './outputs/best_model_parameters.joblib')\n",
    "\n",
    "print(\"The best model has been saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: AutoML modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TabularDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.data.dataset_factory import TabularDatasetFactory\n",
    "\n",
    "# Create TabularDataset using TabularDatasetFactory\n",
    "# Data is available at: \n",
    "# \"https://automlsamplenotebookdata.blob.core.windows.net/automl-sample-notebook-data/bankmarketing_train.csv\"\n",
    "\n",
    "datapath_url = 'https://automlsamplenotebookdata.blob.core.windows.net/automl-sample-notebook-data/bankmarketing_train.csv'\n",
    "ds = TabularDatasetFactory.from_delimited_files(path = datapath_url, random_state = 42)\n",
    "#ds_name = 'UCbankmarketing'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598275726969
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from train import clean_data\n",
    "if not (pandas in sys.modules):\n",
    "    import pandas as pd\n",
    "\n",
    "# Use the clean_data function to clean the dataset.\n",
    "x, y = clean_data(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into _training_ and _testing_ sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, stratify = y, random_state = 42)\n",
    "ds_train = pd.concat([x_train,y_train], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the AutoML run settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598275665403
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azureml.train.automl import AutoMLConfig\n",
    "\n",
    "# Set parameters for AutoMLConfig\n",
    "# NOTE: DO NOT CHANGE THE 'experiment_timeout_minutes' PARAMETER OR THE INSTANCE WILL TIME OUT.\n",
    "\n",
    "automl_config = AutoMLConfig(\n",
    "    experiment_timeout_minutes = 30,  # DO NOT CHANGE THE 'experiment_timeout_minutes' PARAMETER \n",
    "    enable_early_stopping = True,\n",
    "    debug_log = 'automl_errors.log',\n",
    "    task = 'classification',\n",
    "    primary_metric = 'accuracy',\n",
    "    training_data = ds_train,\n",
    "    label_column_name = 'Subscribe',\n",
    "    n_cross_validations = 6,\n",
    "    compute_target = compute_cluster,\n",
    "    enable_onnx_compatible_models = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the AutoML run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Submit the AutoML run\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "automl_run = exp.submit(config = automl_config, show_output = True)\n",
    "\n",
    "#Launch the widget to view the progress and results\n",
    "RunDetails(automl_run).show()\n",
    "\n",
    "automl_run.wait_for_completion(show_output = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the best AutoML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve and save the best AutoML model.\n",
    "\n",
    "automl_run.get_metrics()\n",
    "print(automl_run.get_portal_url())\n",
    "\n",
    "#Retrieving the best model\n",
    "best_run_AutoML, best_model_AutoML = automl_run.get_output()\n",
    "best_run_metrics_AutoML = best_run_AutoML.get_metrics()\n",
    "\n",
    "print(\"Best AutoML Run Id: \", best_run_AutoML.id)\n",
    "print(\"Accuracy: \", best_run_metrics_AutoML['accuracy'])\n",
    "print(\"Fitted model:\", best_model_AutoML)\n",
    "print(\"Estimator:\", best_model_AutoML._final_estimator)\n",
    "print(\"Other details: \\n\") \n",
    "best_run_AutoML.get_details() \n",
    "best_run_AutoML.get_tags()\n",
    "\n",
    "#Saving the best AutoML model\n",
    "joblib.dump(best_model_AutoML, filename = 'outputs/best_model_AutoML.joblib')\n",
    "\n",
    "model_AutoML_saved = automl_run.register_model(model_name = best_run_AutoML.properties['model_name'], description = 'Best AutoML model')\n",
    "print(\"The best AutoML model has been saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup the compute cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster cleanup\n",
    "\n",
    "compute_cluster.delete()"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "microsoft": {
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
